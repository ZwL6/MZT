{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ+e4UiJwcKng+moTpGks5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZwL6/MZT/blob/main/assignment-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vgGYzsiFZyE",
        "outputId": "03d42560-6cca-4e89-a56e-efb78b3d67b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=73a5c5b4a30fab4bcf5ebb4b674fdf7e7fbf25cca09daa3bfd8ba76e1c3ed6bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 123069 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "nS9UM1hoFizF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cities file\n",
        "!wget https://raw.githubusercontent.com/ZwL6/MZT/main/cities.json\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcIZ1cyMJPs0",
        "outputId": "ab7191a9-6aeb-483a-b567-f03632715cd1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-02 16:52:51--  https://raw.githubusercontent.com/ZwL6/MZT/main/cities.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15829 (15K) [text/plain]\n",
            "Saving to: ‘cities.json’\n",
            "\n",
            "\rcities.json           0%[                    ]       0  --.-KB/s               \rcities.json         100%[===================>]  15.46K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-07-02 16:52:51 (30.0 MB/s) - ‘cities.json’ saved [15829/15829]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#making standerd route\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Set seed\n",
        "random.seed(1)\n",
        "with open('cities.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "cities = [d['city'] for d in data]\n",
        "\n",
        "merchandise = ['apple', 'tomatoes', 'butter', 'water', 'milk', 'pens', 'honey']\n",
        "\n",
        "routes = []\n",
        "\n",
        "for i in range(1000):\n",
        "    route = {}\n",
        "    route['id'] = i\n",
        "\n",
        "    # Generate a list of non-repeating random cities\n",
        "    city_sample = random.sample(cities, 3)\n",
        "\n",
        "    # Randomly generate the number of merchandise\n",
        "    num_merchandise = random.randint(1, 10)\n",
        "\n",
        "    # Randomly select merchandise types\n",
        "    merchandise_sample = random.sample(merchandise, random.randint(3, 5))\n",
        "\n",
        "    # Build the route dictionary format\n",
        "    route['route'] = []\n",
        "    for j in range(2):\n",
        "        route_info = {}\n",
        "        route_info['from'] = city_sample[j]\n",
        "        route_info['to'] = city_sample[j+1]\n",
        "\n",
        "        # Generate random merchandise quantities for each route\n",
        "        merchandise_dict = {}\n",
        "        for m in merchandise_sample:\n",
        "            merchandise_dict[m] = random.randint(1, num_merchandise)\n",
        "\n",
        "        route_info['merchandise'] = merchandise_dict\n",
        "\n",
        "        route['route'].append(route_info)\n",
        "\n",
        "    routes.append(route)\n",
        "\n",
        "# Write the route list to a JSON file without corrupt records\n",
        "valid_routes = [r for r in routes if 'route' in r and len(r['route']) == 2]\n",
        "with open('routes.json', 'w') as file:\n",
        "    json.dump(routes, file, indent=4)\n"
      ],
      "metadata": {
        "id": "qj8cYyhKSHBl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download routes\n",
        "# from google.colab import files\n",
        "# files.download('routes.json')\n"
      ],
      "metadata": {
        "id": "uNZs3ShmVCAl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making actual route\n",
        "\n",
        "import json\n",
        "import random\n",
        "import copy\n",
        "\n",
        "# Set seed\n",
        "random.seed(2)\n",
        "\n",
        "# Read the JSON file\n",
        "with open('cities.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract city names\n",
        "cities = [item['city'] for item in data]\n",
        "\n",
        "# Define merchandise types\n",
        "merchandise_types = ['apple', 'tomatoes', 'butter', 'water', 'milk', 'pens', 'honey']\n",
        "\n",
        "with open('routes.json', 'r') as file:\n",
        "    routes = json.load(file)\n",
        "\n",
        "act_routes = []\n",
        "\n",
        "for route in routes:\n",
        "    if len(route['route']) < 2:\n",
        "        continue  # Skip routes with less than two cities\n",
        "\n",
        "    for _ in range(10):\n",
        "        act_route = copy.deepcopy(route)  # Use deepcopy to avoid modifying the original route\n",
        "\n",
        "        # Randomly select a city to modify\n",
        "        modified_city_index = random.randint(0, len(act_route['route']) - 1)\n",
        "        modified_city = act_route['route'][modified_city_index]\n",
        "\n",
        "        # Randomly decide whether to modify city\n",
        "        if random.choice([True, False]):\n",
        "            # Modify the selected city\n",
        "            new_city = random.choice(cities)\n",
        "            modified_city['from'] = new_city\n",
        "\n",
        "            # If the modified city is the second city, modify it throughout the route\n",
        "            if modified_city_index == 1:\n",
        "                # Replace the modified city with a new city\n",
        "                act_route['route'][modified_city_index]['from'] = new_city\n",
        "                # Connect the cities before and after the replaced city\n",
        "                act_route['route'][modified_city_index - 1]['to'] = new_city\n",
        "                if modified_city_index + 1 < len(act_route['route']):\n",
        "                    act_route['route'][modified_city_index]['to'] = act_route['route'][modified_city_index + 1]['from']\n",
        "\n",
        "        # Randomly modify merchandise\n",
        "        for city_route in act_route['route']:\n",
        "          # Randomly decide whether to modify merchandise\n",
        "          if random.choice([True, False]):\n",
        "            # Select a random merchandise type\n",
        "            modified_merchandise = random.choice(list(city_route['merchandise'].keys()))\n",
        "            # Modify the quantity or replace the merchandise type\n",
        "            if random.choice([True, False]):\n",
        "              # Modify the quantity\n",
        "              city_route['merchandise'][modified_merchandise] = random.randint(1, 5)\n",
        "            else:\n",
        "              # Replace the merchandise type\n",
        "              new_merchandise = random.choice(merchandise_types)\n",
        "              if modified_merchandise in city_route['merchandise']:\n",
        "                city_route['merchandise'][new_merchandise] = city_route['merchandise'][modified_merchandise]\n",
        "                del city_route['merchandise'][modified_merchandise]\n",
        "\n",
        "\n",
        "        # Randomly modify merchandise\n",
        "        #for city_route in act_route['route']:\n",
        "            # Select random merchandise types\n",
        "            #selected_merchandise = random.sample(merchandise_types, random.randint(1, len(merchandise_types)))\n",
        "            # Assign random quantities to selected merchandise\n",
        "            #city_route['merchandise'] = {item: random.randint(1, 5) for item in selected_merchandise}\n",
        "\n",
        "        act_routes.append(act_route)\n",
        "\n",
        "# Write the act routes list to a new JSON file\n",
        "with open('act_routes.json', 'w') as file:\n",
        "    json.dump(act_routes, file, indent=4)"
      ],
      "metadata": {
        "id": "CGJurVstWvbv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the actual routes\n",
        "# from google.colab import files\n",
        "# files.download('act_routes.json')"
      ],
      "metadata": {
        "id": "WOqkwzTkX1L5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-Levenshtein\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from Levenshtein import distance\n",
        "\n",
        "# Function for calculating route edit distance similarity\n",
        "def calculate_similarity(route1, route2):\n",
        "    lev_distance = distance(route1, route2)\n",
        "    max_length = max(len(route1), len(route2))\n",
        "    similarity = 1 - lev_distance / max_length\n",
        "    return similarity\n",
        "\n",
        "# Function for calculating marchandise edit distance similarity\n",
        "def calculate_merchandise_similarity(merch1, merch2):\n",
        "    lev_distance = distance(merch1, merch2)\n",
        "    max_length = max(len(merch1), len(merch2))\n",
        "    similarity = 1 - lev_distance / max_length\n",
        "    return similarity\n",
        "\n",
        "# open standard route file\n",
        "with open('routes.json', 'r') as file:\n",
        "    routes = json.load(file)\n",
        "\n",
        "# open actual route file\n",
        "with open('act_routes.json', 'r') as file:\n",
        "    act_routes = json.load(file)\n",
        "\n",
        "# Create a dictionary to store the most similar route for each actual route\n",
        "most_similar_routes = defaultdict(list)\n",
        "\n",
        "# Correct prediction counter\n",
        "correct_predictions = 0\n",
        "\n",
        "# Iterate through all actual routes\n",
        "for act_route in act_routes:\n",
        "    act_route_id = act_route['id']\n",
        "    act_route_str = ' '.join([f\"{route['from']} {route['to']}\" for route in act_route['route']])\n",
        "\n",
        "    similarities = []\n",
        "\n",
        "    # route similarity calculating\n",
        "    for standard_route in routes:\n",
        "        standard_route_id = standard_route['id']\n",
        "        standard_route_str = ' '.join([f\"{route['from']} {route['to']}\" for route in standard_route['route']])\n",
        "\n",
        "        route_similarity = calculate_similarity(standard_route_str, act_route_str)\n",
        "\n",
        "        # merchandise similarity calculating\n",
        "        merchandise_similarity = 0\n",
        "        for i, route in enumerate(act_route['route']):\n",
        "            act_merchandise = route['merchandise']\n",
        "            standard_merchandise = standard_route['route'][0]['merchandise']\n",
        "            merchandise_similarity += calculate_merchandise_similarity(str(standard_merchandise), str(act_merchandise))\n",
        "        merchandise_similarity /= len(standard_route['route'])\n",
        "\n",
        "        #total similarity calculating\n",
        "        total_similarity = (route_similarity + merchandise_similarity) / 2\n",
        "        similarities.append((standard_route_id, total_similarity))\n",
        "\n",
        "    # sort the max similarity\n",
        "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Find the most similar route\n",
        "    max_similarity = similarities[0]\n",
        "\n",
        "    # Store the most similar route for this actual route\n",
        "    most_similar_routes[act_route_id].append(max_similarity)\n",
        "\n",
        "    # If the actual route id is the same as the predicted route id, increment the correct prediction counter\n",
        "    if act_route_id == max_similarity[0]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "# Print the most similar routes for all actual routes\n",
        "# for act_route_id, max_similarities in most_similar_routes.items():\n",
        "#    for i, max_similarity in enumerate(max_similarities):\n",
        "#        print(f\"Max similarity for actual route {act_route_id}-{i+1}: Standard Route {max_similarity[0]} with similarity {max_similarity[1]}\")\n",
        "\n",
        "# Calculate and print the prediction accuracy\n",
        "accuracy = correct_predictions / len(act_routes)\n",
        "print(f\"Prediction accuracy: {accuracy * 100}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdfcXzp1kVSA",
        "outputId": "c652cd05-f951-453d-cf1f-e9ef3ff7e80c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.21.1-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.21.1 (from python-Levenshtein)\n",
            "  Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein==0.21.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.21.1 python-Levenshtein-0.21.1 rapidfuzz-3.1.1\n",
            "Prediction accuracy: 99.2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'standard routes' file to Spark DataFrame\n",
        "import pandas as pd\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load data from JSON file\n",
        "with open('routes.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Initialize an empty DataFrame to store the processed data\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Iterate over each element\n",
        "for element in data:\n",
        "  id = element['id']\n",
        "\n",
        "# Iterate over routes\n",
        "  for route in element['route']:\n",
        "    from_city = route['from']\n",
        "    to_city = route['to']\n",
        "\n",
        "    # Iterate over merchandise\n",
        "    for merchandise, quantity in route['merchandise'].items():\n",
        "        # Append data to DataFrame\n",
        "        df = df.append({\n",
        "            'id': id,\n",
        "            'from': from_city,\n",
        "            'to': to_city,\n",
        "            'merchandise': merchandise,\n",
        "            'quantity': quantity\n",
        "        }, ignore_index=True)\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Convert DataFrame to Spark DataFrame\n",
        "st_routes_df = spark.createDataFrame(df)"
      ],
      "metadata": {
        "id": "uoS_LRqHXCNl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Create a new column that is the merchandise column value repeated quantity times\n",
        "str_df = st_routes_df.withColumn('merchandise_quantity', expr(\"repeat(merchandise, quantity)\"))\n",
        "str_df = str_df.drop('merchandise','quantity')\n",
        "\n",
        "str_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da0fAyHq0Q4J",
        "outputId": "f878b0d1-a9b7-4d37-eb39-35461eb68540"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+-----------+--------------------+\n",
            "| id|       from|         to|merchandise_quantity|\n",
            "+---+-----------+-----------+--------------------+\n",
            "|  0|    Helmond|Schoonhoven|waterwaterwaterwater|\n",
            "|  0|    Helmond|Schoonhoven|          honeyhoney|\n",
            "|  0|    Helmond|Schoonhoven|                pens|\n",
            "|  0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "|  0|Schoonhoven|    Zaandam|               honey|\n",
            "+---+-----------+-----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STANDERD ROUTE\n",
        "from pyspark.sql.functions import concat_ws, collect_list\n",
        "\n",
        "# Group the dataframe by 'id', 'from', and 'to' and concatenate the 'merchandise_quantity' strings\n",
        "st_df = str_df.groupBy('id', 'from', 'to').agg(concat_ws('', collect_list('merchandise_quantity')).alias('combined_merchandise'))\n",
        "\n",
        "st_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkN20_DNp7ft",
        "outputId": "0565d0e0-2484-4930-b87b-cb4ef3d5ddc6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+------------+--------------------+\n",
            "| id|        from|          to|combined_merchandise|\n",
            "+---+------------+------------+--------------------+\n",
            "|  0|     Helmond| Schoonhoven|waterwaterwaterwa...|\n",
            "|  0| Schoonhoven|     Zaandam|waterwaterwaterwa...|\n",
            "|  1|Klazienaveen|   The Hague|penstomatoestomat...|\n",
            "|  1|      Nuenen|Klazienaveen|penspenspenspensp...|\n",
            "|  2|    Enschede|   Harlingen|milkmilkmilkmilkt...|\n",
            "+---+------------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "# Load data from JSON file\n",
        "with open('act_routes.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Initialize an empty DataFrame to store the processed data\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Dictionary to store the counter for each unique ID\n",
        "id_counter = {}\n",
        "\n",
        "# Iterate over each element\n",
        "for element in data:\n",
        "    id = element['id']\n",
        "\n",
        "    # Increment the counter for each unique ID\n",
        "    if id in id_counter:\n",
        "        id_counter[id] += 1\n",
        "    else:\n",
        "        id_counter[id] = 1\n",
        "\n",
        "    # Counter for each ID\n",
        "    counter = id_counter[id]\n",
        "\n",
        "    # Iterate over routes\n",
        "    for route in element['route']:\n",
        "        from_city = route['from']\n",
        "        to_city = route['to']\n",
        "\n",
        "        # Iterate over merchandise\n",
        "        for merchandise, quantity in route['merchandise'].items():\n",
        "            # Append data to DataFrame with modified ID\n",
        "            df = df.append({\n",
        "                'ac_id': f\"{id}-{counter}\",  # Assign modified ID with counter\n",
        "                'original_id': id,  # Store original ID\n",
        "                'from': from_city,\n",
        "                'to': to_city,\n",
        "                'merchandise': merchandise,\n",
        "                'quantity': quantity\n",
        "            }, ignore_index=True)\n",
        "\n",
        "# Convert DataFrame to Spark DataFrame\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "ac_routes_df = spark.createDataFrame(df)\n",
        "\n",
        "# Assign sequential counts to duplicate IDs\n",
        "ac_routes_df = ac_routes_df.withColumn('new_id', monotonically_increasing_id())\n"
      ],
      "metadata": {
        "id": "EpXKTrpdhTib"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Calculate the sum of quantity for each ac_id and original_id\n",
        "acpath_df = ac_routes_df.groupBy('ac_id', 'original_id').agg(F.sum('quantity').alias('ac_quantity'))\n",
        "\n",
        "# Sort the DataFrame by ac_id\n",
        "acpath_df = acpath_df.orderBy('ac_id')\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "acpath_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbP87pfO8Akh",
        "outputId": "a6ec06d9-acfd-4bce-c374-9b02c0f396f1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+-----------+\n",
            "|ac_id|original_id|ac_quantity|\n",
            "+-----+-----------+-----------+\n",
            "|  0-1|          0|         16|\n",
            "| 0-10|          0|         16|\n",
            "|  0-2|          0|         14|\n",
            "|  0-3|          0|         12|\n",
            "|  0-4|          0|         16|\n",
            "|  0-5|          0|         12|\n",
            "|  0-6|          0|         16|\n",
            "|  0-7|          0|         12|\n",
            "|  0-8|          0|         16|\n",
            "|  0-9|          0|         19|\n",
            "|  1-1|          1|         26|\n",
            "| 1-10|          1|         21|\n",
            "|  1-2|          1|         26|\n",
            "|  1-3|          1|         22|\n",
            "|  1-4|          1|         26|\n",
            "|  1-5|          1|         27|\n",
            "|  1-6|          1|         27|\n",
            "|  1-7|          1|         28|\n",
            "|  1-8|          1|         28|\n",
            "|  1-9|          1|         28|\n",
            "+-----+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combine merchandies and quantity\n",
        "acr_df = ac_routes_df.withColumn('merchandise_quantity', expr(\"repeat(merchandise, quantity)\"))\n",
        "acr_df = acr_df.drop('merchandise', 'quantity', 'new_id')\n",
        "\n",
        "acr_df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jcEdj7rKeBB",
        "outputId": "ed24fe27-738a-4032-992e-f9ad016d1b48"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+-----------+-----------+--------------------+\n",
            "|ac_id|original_id|       from|         to|merchandise_quantity|\n",
            "+-----+-----------+-----------+-----------+--------------------+\n",
            "|  0-1|          0|Barendrecht|Schoonhoven|waterwaterwaterwater|\n",
            "|  0-1|          0|Barendrecht|Schoonhoven|          honeyhoney|\n",
            "|  0-1|          0|Barendrecht|Schoonhoven|                pens|\n",
            "|  0-1|          0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "|  0-1|          0|Schoonhoven|    Zaandam|               honey|\n",
            "+-----+-----------+-----------+-----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ACTUAL ROUTE\n",
        "from pyspark.sql.functions import first\n",
        "\n",
        "# Group the dataframe by 'id', 'original_id', 'from', and 'to' and concatenate the 'merchandise_quantity' strings\n",
        "ac_df = acr_df.groupby('ac_id', 'original_id', 'from', 'to').agg(concat_ws(' ', collect_list('merchandise_quantity')).alias('combined_merchandise'))\n",
        "\n",
        "ac_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu6MY-JB5U27",
        "outputId": "ba3fcb76-5884-47ae-feee-15b5aec03cf7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+-----------+-----------+--------------------+\n",
            "|ac_id|original_id|       from|         to|combined_merchandise|\n",
            "+-----+-----------+-----------+-----------+--------------------+\n",
            "|  0-1|          0|Schoonhoven|    Zaandam|waterwaterwaterwa...|\n",
            "|  0-2|          0|     Bergen|Schoonhoven|waterwaterwaterwa...|\n",
            "|  0-4|          0|    Helmond|Schoonhoven|waterwaterwaterwa...|\n",
            "|  0-5|          0|Schoonhoven|    Zaandam|waterwaterwaterwa...|\n",
            "|  0-5|          0|  Staphorst|Schoonhoven|waterwaterwaterwa...|\n",
            "+-----+-----------+-----------+-----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-Levenshtein\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType, col\n",
        "import pandas as pd\n",
        "import Levenshtein\n",
        "\n",
        "# Define a pandas UDF to calculate similarity\n",
        "@pandas_udf(returnType=FloatType(), functionType=PandasUDFType.SCALAR)\n",
        "def calculate_similarity(str1: pd.Series, str2: pd.Series) -> pd.Series:\n",
        "    similarity = str1.combine(str2, lambda s1, s2: 1 - Levenshtein.distance(s1, s2) / max(len(s1), len(s2)))\n",
        "    return similarity\n",
        "\n",
        "# Rename the columns 'from' and 'to' in ac_df and st_df\n",
        "ac_df = ac_df.withColumnRenamed('from', 'ac_from').withColumnRenamed('to', 'ac_to')\n",
        "st_df = st_df.withColumnRenamed('from', 'st_from').withColumnRenamed('to', 'st_to')\n",
        "a_df = ac_df.select(col('ac_id'), col('original_id'), col('ac_from'), col('ac_to'), col('combined_merchandise'))\n",
        "s_df = st_df.select(col('id'), col('st_from'), col('st_to'), col('combined_merchandise'))\n",
        "\n",
        "# Combine the actual routes with the same act_id\n",
        "a_df = a_df.groupBy('ac_id').agg(F.first('original_id').alias('original_id'), F.concat_ws(' ', F.collect_list('ac_from'), F.collect_list('ac_to'), F.collect_list('combined_merchandise')).alias('combined_route'))\n",
        "\n",
        "# Combine the standard routes with the same id\n",
        "s_df = s_df.groupBy('id').agg(F.concat_ws(' ', F.collect_list('st_from'), F.collect_list('st_to'), F.collect_list('combined_merchandise')).alias('st_combined_route'))\n",
        "\n",
        "# Calculate similarity using pandas UDF\n",
        "ac_st_df = a_df.crossJoin(s_df).withColumn('similarity', calculate_similarity(col('combined_route'), col('st_combined_route')))\n",
        "\n",
        "# Find the most similar route for each actual route\n",
        "window = Window.partitionBy(ac_st_df['ac_id']).orderBy(ac_st_df['similarity'].desc())\n",
        "most_similar_routes = ac_st_df.withColumn('rank', F.row_number().over(window)).filter(col('rank') == 1).select('original_id', 'id', 'similarity')\n",
        "\n",
        "# Count the correct predictions\n",
        "correct_predictions = most_similar_routes.filter(col('original_id') == col('id')).count()\n",
        "\n",
        "# Calculate and print the prediction accuracy\n",
        "accuracy = correct_predictions / ac_df.count()\n",
        "print(f\"Prediction accuracy: {accuracy * 100}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqy531KZLw6M",
        "outputId": "6862c7b4-9655-444f-a8ce-a691f55c8694"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (0.21.1)\n",
            "Requirement already satisfied: Levenshtein==0.21.1 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein) (0.21.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.21.1->python-Levenshtein) (3.1.1)\n",
            "Prediction accuracy: 41.715%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar_routes = ac_st_df.withColumn('rank', F.row_number().over(window)).filter(col('rank') == 1).select('ac_id', 'similarity')\n",
        "most_similar_routes.show(11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W5k542m--op",
        "outputId": "235e1409-bc1b-415f-de13-2711320bee19"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "|ac_id|similarity|\n",
            "+-----+----------+\n",
            "|  0-8|0.56666666|\n",
            "|  0-9|0.85925925|\n",
            "|  1-1| 0.8367347|\n",
            "|  1-2| 0.9132653|\n",
            "|  1-3| 0.5102041|\n",
            "|  1-7| 0.5648148|\n",
            "|  1-8|0.97029704|\n",
            "|10-10| 0.6280992|\n",
            "| 10-6|0.57024795|\n",
            "| 10-7| 0.8429752|\n",
            "| 10-8| 0.5068493|\n",
            "+-----+----------+\n",
            "only showing top 11 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar_routes.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU2cu5wFyqm_",
        "outputId": "c6da176a-2365-4176-80a8-75feb5cf5183"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the acpath DataFrame with most_similar_routes based on ac_id\n",
        "acpath_with_similarity = acpath_df.join(most_similar_routes, on='ac_id', how='left')\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "acpath_with_similarity.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6szD_QS9a_z",
        "outputId": "7cd33afc-9cf3-4ea9-e4cd-c67f486010ab"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+-----------+----------+\n",
            "| ac_id|original_id|ac_quantity|similarity|\n",
            "+------+-----------+-----------+----------+\n",
            "|117-10|        117|         44| 0.5745192|\n",
            "| 118-8|        118|         28| 0.6089386|\n",
            "| 200-5|        200|         27| 0.5804598|\n",
            "| 209-6|        209|          9|       0.5|\n",
            "| 220-1|        220|         10| 0.8043478|\n",
            "| 225-6|        225|         16|0.70542634|\n",
            "| 264-5|        264|          6|       0.6|\n",
            "| 278-2|        278|         10| 0.7234042|\n",
            "| 290-8|        290|         16| 0.9054054|\n",
            "|  32-6|         32|         24|0.61403507|\n",
            "|  62-1|         62|         30| 0.7821782|\n",
            "|  66-5|         66|          6| 0.5135135|\n",
            "| 101-8|        101|         27| 0.9268293|\n",
            "|  11-3|         11|         23| 0.5620915|\n",
            "| 126-2|        126|         13| 0.8956522|\n",
            "|  18-1|         18|         18| 0.7222222|\n",
            "| 214-7|        214|         26|0.86413044|\n",
            "| 222-7|        222|         17|0.95620435|\n",
            "| 225-3|        225|         13| 0.6764706|\n",
            "|  39-2|         39|         32| 0.9067358|\n",
            "+------+-----------+-----------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "BONUS_AMOUNT = 1000\n",
        "ROUTE_AMOUNT = 250\n",
        "MERCHANDISE_RATE = 2\n",
        "BANK_ACCOUNT_LIMIT = 500000\n",
        "FINE_PERCENTAGE = 0.05\n",
        "\n",
        "from pyspark.sql.functions import lit, sum as Fsum, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Calculate payment for each driver in actual routes\n",
        "def calculate_payment(act_routes):\n",
        "    # Calculate the payment using Spark DataFrame functions\n",
        "    act_routes = act_routes.withColumn('payment', (act_routes['similarity'] * lit(BONUS_AMOUNT)) + lit(ROUTE_AMOUNT * 2) + (act_routes['ac_quantity'] * lit(MERCHANDISE_RATE)))\n",
        "    total_payment = act_routes.select(Fsum('payment')).first()[0]\n",
        "\n",
        "    return total_payment, act_routes\n",
        "\n",
        "# Optimize payment scheme\n",
        "def optimize_payment(act_routes):\n",
        "    # Create a window specification\n",
        "    window = Window.orderBy(act_routes['payment'].desc())\n",
        "\n",
        "    # Add a cumulative sum column\n",
        "    act_routes = act_routes.withColumn('cumulative_payment', Fsum('payment').over(window))\n",
        "\n",
        "    # Initialize lists to store deposit back amounts and fines\n",
        "    deposit_back_amounts = []\n",
        "    fines = []\n",
        "\n",
        "    # Calculate the number of routes that can be paid\n",
        "    while act_routes.count() > 0:\n",
        "        max_payment = act_routes.filter(act_routes['cumulative_payment'] <= BANK_ACCOUNT_LIMIT).select(Fsum('payment')).first()[0]\n",
        "\n",
        "        # Filter the routes that can be paid\n",
        "        paid_routes = act_routes.filter(act_routes['payment'] <= max_payment)\n",
        "\n",
        "        # Calculate the deposit back amount\n",
        "        deposit_back = BANK_ACCOUNT_LIMIT - max_payment\n",
        "        deposit_back_amounts.append(deposit_back)\n",
        "\n",
        "        # Calculate the fine amount\n",
        "        fine = deposit_back * FINE_PERCENTAGE\n",
        "        fines.append(fine)\n",
        "\n",
        "        # Remove paid routes\n",
        "        act_routes = act_routes.subtract(paid_routes)\n",
        "\n",
        "    return deposit_back_amounts, fines, paid_routes\n",
        "\n",
        "# Calculate total payment amount and update the DataFrame with 'payment' column\n",
        "total_payment, acpath_with_similarity = calculate_payment(acpath_with_similarity)\n",
        "\n",
        "# Optimize payment scheme and get deposit back amounts, fine amount, and payment history\n",
        "deposit_back_amounts, fines, paid_routes = optimize_payment(acpath_with_similarity)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Total payment amount: ${total_payment:.2f}\")\n",
        "print(f\"Total deposit back amount: ${sum(deposit_back_amounts):.2f}\")\n",
        "print(f\"Total fine amount: ${sum(fines):.2f}\")\n",
        "\n",
        "# Print the fine for each deposit\n",
        "for i, (deposit, fine) in enumerate(zip(deposit_back_amounts, fines)):\n",
        "    print(f\"Fine for deposit {i+1}: ${fine:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUdC8z1Ybxi9",
        "outputId": "3c05a34f-a14a-4369-a973-bdc4cb21866b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total payment amount: $13051630.07\n",
            "Total deposit back amount: $120.42\n",
            "Total fine amount: $6.02\n",
            "Fine for deposit 1: $6.02\n"
          ]
        }
      ]
    }
  ]
}