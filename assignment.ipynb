{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwdotfj0sfsUd40JvK3gE6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZwL6/MZT/blob/main/assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vgGYzsiFZyE",
        "outputId": "b8d377b8-bc47-4191-82c3-09d1b6976565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=d955569559477ea5a280cd155b6859c441fc028a303d4056751b159dfd4f42c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 123069 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "nS9UM1hoFizF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cities file\n",
        "!wget https://raw.githubusercontent.com/ZwL6/MZT/main/cities.json\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcIZ1cyMJPs0",
        "outputId": "6b2dccc5-19dc-4207-9adc-c630c35e39b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-30 13:27:34--  https://raw.githubusercontent.com/ZwL6/MZT/main/cities.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15829 (15K) [text/plain]\n",
            "Saving to: ‘cities.json’\n",
            "\n",
            "\rcities.json           0%[                    ]       0  --.-KB/s               \rcities.json         100%[===================>]  15.46K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-30 13:27:34 (142 MB/s) - ‘cities.json’ saved [15829/15829]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#making standerd route\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Set seed\n",
        "random.seed(1)\n",
        "with open('cities.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "cities = [d['city'] for d in data]\n",
        "\n",
        "merchandise = ['apple', 'tomatoes', 'butter', 'water', 'milk', 'pens', 'honey']\n",
        "\n",
        "routes = []\n",
        "\n",
        "for i in range(1000):\n",
        "    route = {}\n",
        "    route['id'] = i\n",
        "\n",
        "    # Generate a list of non-repeating random cities\n",
        "    city_sample = random.sample(cities, 3)\n",
        "\n",
        "    # Randomly generate the number of merchandise\n",
        "    num_merchandise = random.randint(1, 10)\n",
        "\n",
        "    # Randomly select merchandise types\n",
        "    merchandise_sample = random.sample(merchandise, random.randint(3, 5))\n",
        "\n",
        "    # Build the route dictionary format\n",
        "    route['route'] = []\n",
        "    for j in range(2):\n",
        "        route_info = {}\n",
        "        route_info['from'] = city_sample[j]\n",
        "        route_info['to'] = city_sample[j+1]\n",
        "\n",
        "        # Generate random merchandise quantities for each route\n",
        "        merchandise_dict = {}\n",
        "        for m in merchandise_sample:\n",
        "            merchandise_dict[m] = random.randint(1, num_merchandise)\n",
        "\n",
        "        route_info['merchandise'] = merchandise_dict\n",
        "\n",
        "        route['route'].append(route_info)\n",
        "\n",
        "    routes.append(route)\n",
        "\n",
        "# Write the route list to a JSON file without corrupt records\n",
        "valid_routes = [r for r in routes if 'route' in r and len(r['route']) == 2]\n",
        "with open('routes.json', 'w') as file:\n",
        "    json.dump(routes, file, indent=4)\n"
      ],
      "metadata": {
        "id": "qj8cYyhKSHBl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download routes\n",
        "# from google.colab import files\n",
        "# files.download('routes.json')\n"
      ],
      "metadata": {
        "id": "uNZs3ShmVCAl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making actual route\n",
        "\n",
        "import json\n",
        "import random\n",
        "import copy\n",
        "\n",
        "# Set seed\n",
        "random.seed(2)\n",
        "\n",
        "# Read the JSON file\n",
        "with open('cities.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract city names\n",
        "cities = [item['city'] for item in data]\n",
        "\n",
        "# Define merchandise types\n",
        "merchandise_types = ['apple', 'tomatoes', 'butter', 'water', 'milk', 'pens', 'honey']\n",
        "\n",
        "with open('routes.json', 'r') as file:\n",
        "    routes = json.load(file)\n",
        "\n",
        "act_routes = []\n",
        "\n",
        "for route in routes:\n",
        "    if len(route['route']) < 2:\n",
        "        continue  # Skip routes with less than two cities\n",
        "\n",
        "    for _ in range(10):\n",
        "        act_route = copy.deepcopy(route)  # Use deepcopy to avoid modifying the original route\n",
        "\n",
        "        # Randomly select a city to modify\n",
        "        modified_city_index = random.randint(0, len(act_route['route']) - 1)\n",
        "        modified_city = act_route['route'][modified_city_index]\n",
        "\n",
        "        # Randomly decide whether to modify city\n",
        "        if random.choice([True, False]):\n",
        "            # Modify the selected city\n",
        "            new_city = random.choice(cities)\n",
        "            modified_city['from'] = new_city\n",
        "\n",
        "            # If the modified city is the second city, modify it throughout the route\n",
        "            if modified_city_index == 1:\n",
        "                # Replace the modified city with a new city\n",
        "                act_route['route'][modified_city_index]['from'] = new_city\n",
        "                # Connect the cities before and after the replaced city\n",
        "                act_route['route'][modified_city_index - 1]['to'] = new_city\n",
        "                if modified_city_index + 1 < len(act_route['route']):\n",
        "                    act_route['route'][modified_city_index]['to'] = act_route['route'][modified_city_index + 1]['from']\n",
        "\n",
        "        # Randomly modify merchandise\n",
        "        for city_route in act_route['route']:\n",
        "          # Randomly decide whether to modify merchandise\n",
        "          if random.choice([True, False]):\n",
        "            # Select a random merchandise type\n",
        "            modified_merchandise = random.choice(list(city_route['merchandise'].keys()))\n",
        "            # Modify the quantity or replace the merchandise type\n",
        "            if random.choice([True, False]):\n",
        "              # Modify the quantity\n",
        "              city_route['merchandise'][modified_merchandise] = random.randint(1, 5)\n",
        "            else:\n",
        "              # Replace the merchandise type\n",
        "              new_merchandise = random.choice(merchandise_types)\n",
        "              if modified_merchandise in city_route['merchandise']:\n",
        "                city_route['merchandise'][new_merchandise] = city_route['merchandise'][modified_merchandise]\n",
        "                del city_route['merchandise'][modified_merchandise]\n",
        "\n",
        "\n",
        "        # Randomly modify merchandise\n",
        "        #for city_route in act_route['route']:\n",
        "            # Select random merchandise types\n",
        "            #selected_merchandise = random.sample(merchandise_types, random.randint(1, len(merchandise_types)))\n",
        "            # Assign random quantities to selected merchandise\n",
        "            #city_route['merchandise'] = {item: random.randint(1, 5) for item in selected_merchandise}\n",
        "\n",
        "        act_routes.append(act_route)\n",
        "\n",
        "# Write the act routes list to a new JSON file\n",
        "with open('act_routes.json', 'w') as file:\n",
        "    json.dump(act_routes, file, indent=4)"
      ],
      "metadata": {
        "id": "CGJurVstWvbv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the actual routes\n",
        "# from google.colab import files\n",
        "# files.download('act_routes.json')"
      ],
      "metadata": {
        "id": "WOqkwzTkX1L5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-Levenshtein\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from Levenshtein import distance\n",
        "\n",
        "# Function for calculating route edit distance similarity\n",
        "def calculate_similarity(route1, route2):\n",
        "    lev_distance = distance(route1, route2)\n",
        "    max_length = max(len(route1), len(route2))\n",
        "    similarity = 1 - lev_distance / max_length\n",
        "    return similarity\n",
        "\n",
        "# Function for calculating marchandise edit distance similarity\n",
        "def calculate_merchandise_similarity(merch1, merch2):\n",
        "    lev_distance = distance(merch1, merch2)\n",
        "    max_length = max(len(merch1), len(merch2))\n",
        "    similarity = 1 - lev_distance / max_length\n",
        "    return similarity\n",
        "\n",
        "# open standard route file\n",
        "with open('routes.json', 'r') as file:\n",
        "    routes = json.load(file)\n",
        "\n",
        "# open actual route file\n",
        "with open('act_routes.json', 'r') as file:\n",
        "    act_routes = json.load(file)\n",
        "\n",
        "# Create a dictionary to store the most similar route for each actual route\n",
        "most_similar_routes = defaultdict(list)\n",
        "\n",
        "# Correct prediction counter\n",
        "correct_predictions = 0\n",
        "\n",
        "# Iterate through all actual routes\n",
        "for act_route in act_routes:\n",
        "    act_route_id = act_route['id']\n",
        "    act_route_str = ' '.join([f\"{route['from']} {route['to']}\" for route in act_route['route']])\n",
        "\n",
        "    similarities = []\n",
        "\n",
        "    # route similarity calculating\n",
        "    for standard_route in routes:\n",
        "        standard_route_id = standard_route['id']\n",
        "        standard_route_str = ' '.join([f\"{route['from']} {route['to']}\" for route in standard_route['route']])\n",
        "\n",
        "        route_similarity = calculate_similarity(standard_route_str, act_route_str)\n",
        "\n",
        "        # merchandise similarity calculating\n",
        "        merchandise_similarity = 0\n",
        "        for i, route in enumerate(act_route['route']):\n",
        "            act_merchandise = route['merchandise']\n",
        "            standard_merchandise = standard_route['route'][0]['merchandise']\n",
        "            merchandise_similarity += calculate_merchandise_similarity(str(standard_merchandise), str(act_merchandise))\n",
        "        merchandise_similarity /= len(standard_route['route'])\n",
        "\n",
        "        #total similarity calculating\n",
        "        total_similarity = (route_similarity + merchandise_similarity) / 2\n",
        "        similarities.append((standard_route_id, total_similarity))\n",
        "\n",
        "    # sort the max similarity\n",
        "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Find the most similar route\n",
        "    max_similarity = similarities[0]\n",
        "\n",
        "    # Store the most similar route for this actual route\n",
        "    most_similar_routes[act_route_id].append(max_similarity)\n",
        "\n",
        "    # If the actual route id is the same as the predicted route id, increment the correct prediction counter\n",
        "    if act_route_id == max_similarity[0]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "# Print the most similar routes for all actual routes\n",
        "# for act_route_id, max_similarities in most_similar_routes.items():\n",
        "#    for i, max_similarity in enumerate(max_similarities):\n",
        "#        print(f\"Max similarity for actual route {act_route_id}-{i+1}: Standard Route {max_similarity[0]} with similarity {max_similarity[1]}\")\n",
        "\n",
        "# Calculate and print the prediction accuracy\n",
        "accuracy = correct_predictions / len(act_routes)\n",
        "print(f\"Prediction accuracy: {accuracy * 100}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdfcXzp1kVSA",
        "outputId": "631d3467-52c1-4e54-82f1-bba1ae35f870"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.21.1-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.21.1 (from python-Levenshtein)\n",
            "  Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein==0.21.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.21.1 python-Levenshtein-0.21.1 rapidfuzz-3.1.1\n",
            "Prediction accuracy: 99.2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'standard routes' file to Spark DataFrame\n",
        "import pandas as pd\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load data from JSON file\n",
        "with open('routes.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Initialize an empty DataFrame to store the processed data\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Iterate over each element\n",
        "for element in data:\n",
        "  id = element['id']\n",
        "\n",
        "# Iterate over routes\n",
        "  for route in element['route']:\n",
        "    from_city = route['from']\n",
        "    to_city = route['to']\n",
        "\n",
        "    # Iterate over merchandise\n",
        "    for merchandise, quantity in route['merchandise'].items():\n",
        "        # Append data to DataFrame\n",
        "        df = df.append({\n",
        "            'id': id,\n",
        "            'from': from_city,\n",
        "            'to': to_city,\n",
        "            'merchandise': merchandise,\n",
        "            'quantity': quantity\n",
        "        }, ignore_index=True)\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Convert DataFrame to Spark DataFrame\n",
        "st_routes_df = spark.createDataFrame(df)"
      ],
      "metadata": {
        "id": "uoS_LRqHXCNl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Create a new column that is the merchandise column value repeated quantity times\n",
        "str_df = st_routes_df.withColumn('merchandise_quantity', expr(\"repeat(merchandise, quantity)\"))\n",
        "str_df = str_df.drop('merchandise','quantity')\n",
        "\n",
        "str_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da0fAyHq0Q4J",
        "outputId": "d4d6a6e8-8fac-43c6-cc0b-d6ea73517fbb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+-----------+--------------------+\n",
            "| id|       from|         to|merchandise_quantity|\n",
            "+---+-----------+-----------+--------------------+\n",
            "|  0|    Helmond|Schoonhoven|waterwaterwaterwater|\n",
            "|  0|    Helmond|Schoonhoven|          honeyhoney|\n",
            "|  0|    Helmond|Schoonhoven|                pens|\n",
            "|  0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "|  0|Schoonhoven|    Zaandam|               honey|\n",
            "+---+-----------+-----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# delete the duplicated routes\n",
        "stpaths = str_df.dropDuplicates([\"id\", \"from\", \"to\"])\n",
        "\n",
        "stpaths.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHfkTc_kHA5_",
        "outputId": "69622234-1bbc-4dc6-dbd3-e03bf5eaf239"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+------------+--------------------+\n",
            "| id|        from|          to|merchandise_quantity|\n",
            "+---+------------+------------+--------------------+\n",
            "|  0|     Helmond| Schoonhoven|waterwaterwaterwater|\n",
            "|  0| Schoonhoven|     Zaandam|waterwaterwaterwater|\n",
            "|  1|Klazienaveen|   The Hague|                pens|\n",
            "|  1|      Nuenen|Klazienaveen|penspenspenspensp...|\n",
            "|  2|    Enschede|   Harlingen|    milkmilkmilkmilk|\n",
            "+---+------------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STANDERD ROUTE\n",
        "from pyspark.sql.functions import concat_ws, collect_list\n",
        "\n",
        "# Group the dataframe by 'id', 'from', and 'to' and concatenate the 'merchandise_quantity' strings\n",
        "st_df = str_df.groupBy('id', 'from', 'to').agg(concat_ws('', collect_list('merchandise_quantity')).alias('combined_merchandise'))\n",
        "\n",
        "st_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkN20_DNp7ft",
        "outputId": "a39b2475-fefe-42bc-d96d-e8af852c0fe5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+------------+--------------------+\n",
            "| id|        from|          to|combined_merchandise|\n",
            "+---+------------+------------+--------------------+\n",
            "|  0|     Helmond| Schoonhoven|waterwaterwaterwa...|\n",
            "|  0| Schoonhoven|     Zaandam|waterwaterwaterwa...|\n",
            "|  1|Klazienaveen|   The Hague|penstomatoestomat...|\n",
            "|  1|      Nuenen|Klazienaveen|penspenspenspensp...|\n",
            "|  2|    Enschede|   Harlingen|milkmilkmilkmilkt...|\n",
            "+---+------------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "# Load data from JSON file\n",
        "with open('act_routes.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Initialize an empty DataFrame to store the processed data\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Dictionary to store the counter for each unique ID\n",
        "id_counter = {}\n",
        "\n",
        "# Iterate over each element\n",
        "for element in data:\n",
        "    id = element['id']\n",
        "\n",
        "    # Increment the counter for each unique ID\n",
        "    if id in id_counter:\n",
        "        id_counter[id] += 1\n",
        "    else:\n",
        "        id_counter[id] = 1\n",
        "\n",
        "    # Counter for each ID\n",
        "    counter = id_counter[id]\n",
        "\n",
        "    # Iterate over routes\n",
        "    for route in element['route']:\n",
        "        from_city = route['from']\n",
        "        to_city = route['to']\n",
        "\n",
        "        # Iterate over merchandise\n",
        "        for merchandise, quantity in route['merchandise'].items():\n",
        "            # Append data to DataFrame with modified ID\n",
        "            df = df.append({\n",
        "                'id': f\"{id}-{counter}\",  # Assign modified ID with counter\n",
        "                'original_id': id,  # Store original ID\n",
        "                'from': from_city,\n",
        "                'to': to_city,\n",
        "                'merchandise': merchandise,\n",
        "                'quantity': quantity\n",
        "            }, ignore_index=True)\n",
        "\n",
        "# Convert DataFrame to Spark DataFrame\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "ac_routes_df = spark.createDataFrame(df)\n",
        "\n",
        "# Assign sequential counts to duplicate IDs\n",
        "ac_routes_df = ac_routes_df.withColumn('new_id', monotonically_increasing_id())\n"
      ],
      "metadata": {
        "id": "EpXKTrpdhTib"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine merchandies and quantity\n",
        "acr_df = ac_routes_df.withColumn('merchandise_quantity', expr(\"repeat(merchandise, quantity)\"))\n",
        "acr_df = acr_df.drop('merchandise', 'quantity', 'new_id')\n",
        "\n",
        "acr_df.show(20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jcEdj7rKeBB",
        "outputId": "b5ecdec7-2da0-4bd5-e462-335e60f795a0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+-----------+-----------+--------------------+\n",
            "| id|original_id|       from|         to|merchandise_quantity|\n",
            "+---+-----------+-----------+-----------+--------------------+\n",
            "|0-1|          0|Barendrecht|Schoonhoven|waterwaterwaterwater|\n",
            "|0-1|          0|Barendrecht|Schoonhoven|          honeyhoney|\n",
            "|0-1|          0|Barendrecht|Schoonhoven|                pens|\n",
            "|0-1|          0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "|0-1|          0|Schoonhoven|    Zaandam|               honey|\n",
            "|0-1|          0|Schoonhoven|    Zaandam|butterbutterbutte...|\n",
            "|0-2|          0|     Bergen|Schoonhoven|waterwaterwaterwater|\n",
            "|0-2|          0|     Bergen|Schoonhoven|                pens|\n",
            "|0-2|          0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "|0-2|          0|Schoonhoven|    Zaandam|               honey|\n",
            "|0-2|          0|Schoonhoven|    Zaandam|    penspenspenspens|\n",
            "|0-3|          0|    Helmond|Schoonhoven|          honeyhoney|\n",
            "|0-3|          0|    Helmond|Schoonhoven|                pens|\n",
            "|0-3|          0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "|0-3|          0|Schoonhoven|    Zaandam|               honey|\n",
            "|0-3|          0|Schoonhoven|    Zaandam|    penspenspenspens|\n",
            "|0-4|          0|    Helmond|Schoonhoven|waterwaterwaterwater|\n",
            "|0-4|          0|    Helmond|Schoonhoven|          honeyhoney|\n",
            "|0-4|          0|    Helmond|Schoonhoven|            penspens|\n",
            "|0-4|          0|Schoonhoven|    Zaandam|     waterwaterwater|\n",
            "+---+-----------+-----------+-----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ACTUAL ROUTE\n",
        "from pyspark.sql.functions import first\n",
        "\n",
        "# Group the dataframe by 'id', 'original_id', 'from', and 'to' and select the first 'merchandise_quantity' value\n",
        "ac_df = acr_df.groupby('id', 'original_id', 'from', 'to').agg(first('merchandise_quantity').alias('combined_merchandise'))\n",
        "\n",
        "ac_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ich-PRw2rb-f",
        "outputId": "050fe675-3469-4da3-fa4b-a7ec9b93d24e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----------+-----------+-----------+--------------------+\n",
            "|  id|original_id|       from|         to|combined_merchandise|\n",
            "+----+-----------+-----------+-----------+--------------------+\n",
            "| 0-1|          0|Barendrecht|Schoonhoven|waterwaterwaterwater|\n",
            "| 0-1|          0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "|0-10|          0|    Helmond|Schoonhoven|waterwaterwaterwater|\n",
            "|0-10|          0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "| 0-2|          0|     Bergen|Schoonhoven|waterwaterwaterwater|\n",
            "| 0-2|          0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "| 0-3|          0|    Helmond|Schoonhoven|          honeyhoney|\n",
            "| 0-3|          0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "| 0-4|          0|    Helmond|Schoonhoven|waterwaterwaterwater|\n",
            "| 0-4|          0|Schoonhoven|    Zaandam|     waterwaterwater|\n",
            "| 0-5|          0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "| 0-5|          0|  Staphorst|Schoonhoven|waterwaterwaterwater|\n",
            "| 0-6|          0|    Helmond|Schoonhoven|waterwaterwaterwater|\n",
            "| 0-6|          0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "| 0-7|          0|    Helmond|Schoonhoven|waterwaterwaterwater|\n",
            "| 0-7|          0|Schoonhoven|    Zaandam|               water|\n",
            "| 0-8|          0|    Helmond|Schoonhoven|waterwaterwaterwater|\n",
            "| 0-8|          0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "| 0-9|          0|    Helmond|Schoonhoven|waterwaterwaterwater|\n",
            "| 0-9|          0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "+----+-----------+-----------+-----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# delete the duplicated routes\n",
        "acpaths = acr_df.dropDuplicates([\"id\", \"original_id\", \"from\", \"to\"])\n",
        "\n",
        "acpaths.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7ZxDm_V8GkW",
        "outputId": "fa8557a0-fbc3-49d0-d828-576ddc08ad02"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----------+-----------+-----------+--------------------+\n",
            "|  id|original_id|       from|         to|merchandise_quantity|\n",
            "+----+-----------+-----------+-----------+--------------------+\n",
            "| 0-1|          0|Barendrecht|Schoonhoven|waterwaterwaterwater|\n",
            "| 0-1|          0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "|0-10|          0|    Helmond|Schoonhoven|waterwaterwaterwater|\n",
            "|0-10|          0|Schoonhoven|    Zaandam|waterwaterwaterwater|\n",
            "| 0-2|          0|     Bergen|Schoonhoven|waterwaterwaterwater|\n",
            "+----+-----------+-----------+-----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the columns\n",
        "pa_standard = stpaths.selectExpr(\"id as st_id\", \"from as st_from\", \"to as st_to\")\n",
        "pa_actual = acpaths.selectExpr(\"id as act_id\", \"original_id\", \"from as act_from\", \"to as act_to\")\n",
        "\n",
        "pa_actual.show()\n",
        "\n",
        "df_joined = pa_actual.join(pa_standard, (pa_actual.original_id == pa_standard.st_id), 'outer')\n",
        "\n",
        "df_joined.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVmfmim1Fipk",
        "outputId": "9cafb56d-5471-493e-d765-7a8e45ce661e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+---------------+-------------------+\n",
            "|act_id|original_id|       act_from|             act_to|\n",
            "+------+-----------+---------------+-------------------+\n",
            "|   1-8|          1|   Klazienaveen|          The Hague|\n",
            "|  13-3|         13|         Raalte|   Koog aan de Zaan|\n",
            "|  23-7|         23|        Tilburg|           Schinnen|\n",
            "|  32-1|         32|    Heerlerbaan|             Leiden|\n",
            "| 40-10|         40|       Hillegom|        Heerlerbaan|\n",
            "|  77-6|         77|    Oosterwolde|         Oegstgeest|\n",
            "|  82-1|         82|        Leusden|           Ter Apel|\n",
            "|  89-9|         89|        Naarden|             Borger|\n",
            "|  94-8|         94|        Helmond|              Wezep|\n",
            "|135-10|        135|     Teijlingen|          Zandvoort|\n",
            "| 153-7|        153|          Zeist|Hendrik-Ido-Ambacht|\n",
            "| 160-6|        160|        Brummen|        Dedemsvaart|\n",
            "| 173-3|        173|     Schaesberg|                Oss|\n",
            "| 181-7|        181|      Landgraaf|       Monnickendam|\n",
            "| 208-5|        208|     Woudrichem|            Strijen|\n",
            "| 208-8|        208|     Woudrichem|            Strijen|\n",
            "| 212-3|        212|     Zaltbommel|         Doetinchem|\n",
            "| 213-4|        213|     Den Helder|          Barneveld|\n",
            "| 216-2|        216|Sint Willebrord|          Neerijnen|\n",
            "| 224-4|        224|   Badhoevedorp|            Leusden|\n",
            "+------+-----------+---------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+------+-----------+-----------+-----------+-----+-----------+-----------+\n",
            "|act_id|original_id|   act_from|     act_to|st_id|    st_from|      st_to|\n",
            "+------+-----------+-----------+-----------+-----+-----------+-----------+\n",
            "|   0-6|          0|    Helmond|Schoonhoven|    0|    Helmond|Schoonhoven|\n",
            "|   0-6|          0|    Helmond|Schoonhoven|    0|Schoonhoven|    Zaandam|\n",
            "|   0-1|          0|Schoonhoven|    Zaandam|    0|    Helmond|Schoonhoven|\n",
            "|   0-1|          0|Schoonhoven|    Zaandam|    0|Schoonhoven|    Zaandam|\n",
            "|   0-4|          0|    Helmond|Schoonhoven|    0|    Helmond|Schoonhoven|\n",
            "+------+-----------+-----------+-----------+-----+-----------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from itertools import chain\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Load the json file\n",
        "with open('act_routes.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Process the data\n",
        "processed_data = defaultdict(list)\n",
        "for entry in data:\n",
        "    original_id = entry['id']\n",
        "    for i, route in enumerate(entry['route']):\n",
        "        from_city = route['from']\n",
        "        to_city = route['to']\n",
        "        merchandise_list = []\n",
        "        for merchandise, quantity in route['merchandise'].items():\n",
        "            merchandise_list.extend([merchandise]*quantity)\n",
        "        processed_data[original_id].extend([f\"{original_id}-{i}\", from_city, to_city, *merchandise_list])\n",
        "\n",
        "# Convert dictionary to list of tuples\n",
        "processed_data = [tuple(chain(*v)) for v in processed_data.values()]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(processed_data)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zTnuCCTfe4Cl",
        "outputId": "ead8dcb1-5f22-4987-fbf1-9e4da52b619e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-fd4510687ff5>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Show DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o143.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 19) (bc3b1486dd42 executor driver): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 4396 fields are required while 1130 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:802)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 4396 fields are required while 1130 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:802)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "from Levenshtein import distance\n",
        "\n",
        "def levenshtein_distance(s1, s2):\n",
        "    return distance(s1, s2)\n",
        "\n",
        "levenshtein_distance_udf = udf(levenshtein_distance, IntegerType())\n"
      ],
      "metadata": {
        "id": "B4TmnMW2BtIH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}